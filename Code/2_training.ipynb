{"cells":[{"cell_type":"markdown","metadata":{"id":"xTBJmoxaIgI6"},"source":["# 1. Installations & Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0ZP7iwukDGt"},"outputs":[],"source":["!pip install sentencepiece transformers==4.33 datasets wandb sacremoses sacrebleu -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lb3vXtlwkKbu"},"outputs":[],"source":["import logging\n","import os\n","import random\n","import torch\n","import numpy as np\n","import pandas as pd\n","import re\n","import sys\n","import typing as tp\n","import unicodedata\n","import gc\n","import random\n","import sacrebleu\n","import wandb\n","import matplotlib.pyplot as plt\n","from sacremoses import MosesPunctNormalizer\n","from tqdm.auto import tqdm, trange\n","from transformers import NllbTokenizer, AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig, get_constant_schedule_with_warmup\n","from transformers.optimization import Adafactor\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"AdpcI7KSkErj"},"source":["# 2. CHANGE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uvi-3nGeJlAf"},"outputs":[],"source":["# The project folder can be saved in google drive and accessed through Google colab\n","from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqrOwdrfJPPk"},"outputs":[],"source":["#########################\n","#### CHANGE HERE ########\n","#########################\n","\n","# new language tag to add to the model\n","# IMPORTANT: the column in df_train corresponding to this token\n","# should have the same name as new_lang_token\n","new_lang_token = 'ch_be'\n","\n","# Language from which the embeddings should be obtained, can be german or\n","# a dialect the model was trained before\n","# e.g. None, 'deu_Latn', 'ch_vs', 'ch_gr'\n","similar_language_token = 'deu_Latn'\n","\n","# specifiy if the training data for gr should be limited to the training data of vs\n","# this is needed for the model with init_gr_small\n","# only relevant when new_lang_token = 'ch_gr'\n","limit_data = True\n","\n","# path to the project folder that contains the folder data\n","path_project = \"/content/drive/MyDrive/German_to_Swiss_Translation_ANLP_2023\"\n","os.chdir(path_project)\n","\n","# path to the folder where the model is stored\n","# Can be the hugging face model name\n","MODEL_PATH = \"facebook/nllb-200-distilled-600M\"\n","\n","# when the model specified in model_path contains an added token of an added\n","# Swiss german dialect, you need to specify which additional token was added\n","# e.g 'ch_vs' or 'ch_gr'\n","token_added = None\n","\n","# Name/ path to save the finetuned model\n","MODEL_SAVE_PATH = 'model/nllb-de-be_initde_v1'\n","\n","# Training steps\n","# To train the gr-de model large, set training steps to 5000\n","# as it needs more iterations to converge\n","training_steps = 2200\n","\n","# WandB API to log the results, if set to none, no results are logged to wandb\n","api_key = None"]},{"cell_type":"markdown","metadata":{"id":"18wqXYqhKoFf"},"source":["# 3. Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AcP6idZjIom0"},"outputs":[],"source":["def prepare_data(data_path, new_lang_token, limit_data):\n","    \"\"\"\n","    Prepare train and validation data.\n","    \"\"\"\n","    df_train = pd.read_csv(f\"{data_path}/df_train.csv\")\n","    df_test = pd.read_csv(f\"{data_path}/df_test.csv\")\n","\n","    # subset of the data that contains values for this dialect\n","    df_train = df_train[df_train[['de', new_lang_token]].notna().all(axis=1)].reset_index()\n","    df_test = df_test[df_test[['de', new_lang_token]].notna().all(axis=1)].reset_index()\n","\n","    # all dialect exept GR (10475) have between 2700 and 2760 sentences (VS 2619 in train)\n","    # limit the dataframe for GR to 2619 sentences (train) to make it comparable\n","    if limit_data and new_lang_token == 'ch_gr':\n","        df_train = df_train.sample(2619, random_state=42).reset_index(drop=True)\n","\n","    # split into train and validation\n","    df_train, df_val = train_test_split(df_train, test_size=0.05, random_state=42)\n","\n","    print(f\"Length train set: {len(df_train)}\")\n","    print(f\"Length validation: {len(df_val)}\")\n","    print(f\"Length test set: {len(df_test)}\")\n","\n","    return df_train, df_val, df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoDPfDcJLIuT"},"outputs":[],"source":["df_train, df_val, df_test = prepare_data(\"Data/\", new_lang_token, limit_data)"]},{"cell_type":"markdown","metadata":{"id":"Ep_Pmfs9TBEo"},"source":["# 4. Prepare the Model and Tokenizer for the new Language Tag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ffp3noMQIopb"},"outputs":[],"source":["def fix_tokenizer2n(tokenizer, new_lang_tokens):\n","    \"\"\"\n","    Add a new language token to the tokenizer vocabulary,\n","    this should be done each time after its initialization.\n","    This function is used when the model already contains another newly added token\n","    e.g. \"ch_vs\" or \"ch_gr\"\n","    \"\"\"\n","\n","    print(f\"BEFORE IDs: {tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>'])}\")\n","    print(f\"BEFORE Tokens: {tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>']))}\")\n","\n","    # how many of the tokens are actually in tokenizer.added_tokens_encoder\n","    n_added_tokens = len([token for token in new_lang_tokens if token in tokenizer.added_tokens_encoder])\n","\n","    # get the old/ original length of the tokenizer\n","    old_len = len(tokenizer) - n_added_tokens\n","\n","    # move the new tokens in the previous position\n","    for i, token in enumerate(new_lang_tokens):\n","      tokenizer.lang_code_to_id[token] = old_len-i\n","      tokenizer.id_to_lang_code[old_len-old_len-n_added_tokens-i] = token\n","\n","    # always move \"mask\" to the last position\n","    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n","\n","    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n","    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n","\n","    # if the token is not yet trained in the model, add it to the special tokens\n","    for token in new_lang_tokens:\n","      if token not in tokenizer._additional_special_tokens:\n","          tokenizer._additional_special_tokens.append(token)\n","\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    tokenizer.added_tokens_encoder = {}\n","    tokenizer.added_tokens_decoder = {}\n","\n","    print(f\"AFTER IDs: {tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>'])}\")\n","    print(f\"AFTER Tokens: {tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>']))}\")\n","\n","def fix_tokenizer1n(tokenizer, new_lang_token):\n","    \"\"\"\n","    Add a new language token to the tokenizer vocabulary,\n","    this should be done each time after its initialization.\n","    This function is used when the model does not contain another newly added token.\n","    \"\"\"\n","    old_len = len(tokenizer) - int(new_lang_token in tokenizer.added_tokens_encoder)\n","    tokenizer.lang_code_to_id[new_lang_token] = old_len-1\n","    tokenizer.id_to_lang_code[old_len-1] = new_lang_token\n","    # always move \"mask\" to the last position\n","    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n","\n","    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n","    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n","    if new_lang_token not in tokenizer._additional_special_tokens:\n","        tokenizer._additional_special_tokens.append(new_lang_token)\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    tokenizer.added_tokens_encoder = {}\n","    tokenizer.added_tokens_decoder = {}\n","\n","    print(f\"Length of the tokenizer: {len(tokenizer)}\")\n","    print(f\"IDs: {tokenizer.convert_tokens_to_ids([new_lang_token, '<mask>'])}\")\n","    print(f\"IDs: {tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids([new_lang_token, '<mask>']))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTITTab8UM6h"},"outputs":[],"source":["def adapt_embeddings(tokenizer, model, new_lang_token, similar_language_token):\n","  \"\"\"\n","  Rezide the token embeddings of the model, move the embeddings for \"mask\" to it's new position.\n","  If similar_language_token is not None, initialize the new language token with\n","  a token of a similar language.\n","  \"\"\"\n","  added_token_id = tokenizer.convert_tokens_to_ids(new_lang_token)\n","  model.resize_token_embeddings(len(tokenizer))\n","\n","  # moving the embedding for \"mask\" to its new position\n","  model.model.shared.weight.data[added_token_id+1] = model.model.shared.weight.data[added_token_id]\n","\n","  if similar_language_token is not None:\n","    similar_lang_id = tokenizer.convert_tokens_to_ids(similar_language_token)\n","    # initializing new language token with a token of a similar language\n","    model.model.shared.weight.data[added_token_id] = model.model.shared.weight.data[similar_lang_id]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZ6NNz60IosS"},"outputs":[],"source":["tokenizer = NllbTokenizer.from_pretrained(MODEL_PATH)\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n","\n","if MODEL_PATH==\"facebook/nllb-200-distilled-600M\":\n","  # fix tokenizer with new language token\n","  fix_tokenizer1n(tokenizer, new_lang_token)\n","else:\n","  # fix the tokenizer with the added tokens\n","  fix_tokenizer2n(tokenizer, new_lang_tokens=[new_lang_token, token_added])\n","\n","# adapt embeddings\n","adapt_embeddings(tokenizer, model, new_lang_token, similar_language_token)"]},{"cell_type":"markdown","metadata":{"id":"G4yosCJEZQAZ"},"source":["# 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh-4I67ObkVs"},"outputs":[],"source":["def cleanup():\n","    \"\"\"Try to free GPU memory\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","cleanup()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9emWnpL6cTRO"},"outputs":[],"source":["# this code is adapted from https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/monolingual_line_processor.py#L214\n","\n","mpn = MosesPunctNormalizer(lang=\"en\")\n","mpn.substitutions = [\n","    (re.compile(r), sub) for r, sub in mpn.substitutions\n","]\n","\n","def get_non_printing_char_replacer(replace_by: str = \" \") -> tp.Callable[[str], str]:\n","    non_printable_map = {\n","        ord(c): replace_by\n","        for c in (chr(i) for i in range(sys.maxunicode + 1))\n","        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n","        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n","    }\n","\n","    def replace_non_printing_char(line) -> str:\n","        return line.translate(non_printable_map)\n","\n","    return replace_non_printing_char\n","\n","replace_nonprint = get_non_printing_char_replacer(\" \")\n","\n","def preproc(text):\n","    clean = mpn.normalize(text)\n","    clean = replace_nonprint(clean)\n","    clean = unicodedata.normalize(\"NFKC\", clean)\n","    return clean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o77oDq_ibrDF"},"outputs":[],"source":["LANGS = [('de', 'deu_Latn'), (new_lang_token, new_lang_token)]\n","\n","def get_batch_pairs(batch_size, data=df_train):\n","    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n","    xx, yy = [], []\n","    for _ in range(batch_size):\n","        item = data.iloc[random.randint(0, len(data)-1)]\n","        xx.append(preproc(item[l1]))\n","        yy.append(preproc(item[l2]))\n","    return xx, yy, long1, long2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-w2WHpYT556"},"outputs":[],"source":["# model specific parameters\n","batch_size = 16\n","max_length = 128\n","warmup_steps = 1000\n","scale_parameter=False\n","relative_step=False\n","lr=1e-4\n","clip_threshold=1.0\n","weight_decay=1e-3\n","\n","if api_key is not None:\n","\n","  project = \"nllb-200\" # don't need to change\n","  name = f'{MODEL_SAVE_PATH.split(\"/\")[-1]}'\n","\n","  !wandb login {api_key}\n","\n","  # start a new wandb run to track this script\n","  wandb.init(\n","      # set the wandb project where this run will be logged\n","      project=project,\n","      name=name,\n","      save_code=True, # save the main script or notebook to W&B\n","      # track hyperparameters and run metadata\n","      config={\n","        \"architecture\": \"nllb-200\",\n","        \"dataset\": \"Swiss-dial\",\n","        \"batch_size\": batch_size,\n","        \"max_length\": max_length,\n","        \"warmup_steps\": warmup_steps,\n","        \"training_steps\": training_steps,\n","        \"scale_parameter\": scale_parameter,\n","        \"relative_step\": relative_step,\n","        \"lr\": lr,\n","        \"clip_threshold\": clip_threshold,\n","        \"weight_decay\": weight_decay\n","      }\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhcIEtZzT58h"},"outputs":[],"source":["DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","model.to(DEVICE)\n","\n","optimizer = Adafactor(\n","    [p for p in model.parameters() if p.requires_grad],\n","    scale_parameter=scale_parameter,\n","    relative_step=relative_step,\n","    lr=lr,\n","    clip_threshold=clip_threshold,\n","    weight_decay=weight_decay,\n",")\n","\n","losses = []\n","val_losses = []\n","average_train_losses = []\n","average_val_losses = []\n","scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFpAywS6T5_U"},"outputs":[],"source":["# when to save model and printing training and validation loss\n","save_step = 100\n","\n","x, y, loss = None, None, None\n","cleanup()\n","\n","# to store the best model checkpoint\n","best_val_loss = float('inf')  # Initialize with a large value\n","BEST_CHECKPOINT_PATH = f\"{MODEL_SAVE_PATH}_best_checkpoint\"\n","\n","tq = trange(len(losses), training_steps)\n","for i in tq:\n","    ##################\n","    ### TRAIN LOOP ###\n","    ##################\n","    model.train()\n","    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n","    try:\n","        tokenizer.src_lang = lang1\n","        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n","        tokenizer.src_lang = lang2\n","        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n","        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n","        # Forward Pass and get the loss\n","        loss = model(**x, labels=y.input_ids).loss\n","        # backward: perform gradient descent of the loss w.r. to the model params\n","        loss.backward()\n","        # Calculate Loss\n","        losses.append(loss.item())\n","        # Update Weights, update the model parameters by performing a single optimization step\n","        optimizer.step()\n","        # clear the old gradients from optimized variables\n","        optimizer.zero_grad(set_to_none=True)\n","        # Update Weights\n","        scheduler.step()\n","\n","    except RuntimeError as e:\n","        # clear the old gradients from optimized variables\n","        optimizer.zero_grad(set_to_none=True)\n","        x, y, loss = None, None, None\n","        cleanup()\n","        print('error', max(len(s) for s in xx + yy), e)\n","        continue\n","\n","    #######################\n","    ### VALIDATION LOOP ###\n","    #######################\n","\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    # turn off gradients for validation\n","    with torch.no_grad():\n","        xx, yy, lang1, lang2 = get_batch_pairs(batch_size, data=df_val)\n","        tokenizer.src_lang = lang1\n","        x_val = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n","        tokenizer.src_lang = lang2\n","        y_val = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n","        y_val.input_ids[y_val.input_ids == tokenizer.pad_token_id] = -100\n","\n","        val_loss = model(**x_val, labels=y_val.input_ids).loss\n","\n","        val_losses.append(val_loss.item())\n","\n","        # log metrics to wandb\n","        if api_key is not None:\n","          wandb.log({\"loss-valid\": val_loss.item(), \"loss-train\": loss.item()})\n","\n","        model.train()\n","\n","    if i % save_step == 0:\n","        # get the training loss\n","        average_train_loss = np.mean(losses[-save_step:])\n","        average_train_losses.append(average_train_loss)\n","\n","        # get the validation loss\n","        average_val_loss = np.mean(val_losses[-save_step:])\n","        average_val_losses.append(average_val_loss)\n","\n","        print(i, \"| Average train loss: \", average_train_loss, ' | Average validation loss: ', average_val_loss)\n","\n","        # log metrics to wandb\n","        if api_key is not None:\n","          wandb.log({\"loss-valid-average\": average_val_loss, \"loss-train-average\": average_train_loss})\n","\n","        # Check if the current validation loss is the best so far, if yes save the model\n","        if average_val_loss < best_val_loss and i > 0:\n","            best_val_loss = average_val_loss\n","            # Save the best checkpoint\n","            model.save_pretrained(BEST_CHECKPOINT_PATH)\n","            tokenizer.save_pretrained(BEST_CHECKPOINT_PATH)\n","            print(i,f\"| Best model saved at {BEST_CHECKPOINT_PATH}\")\n","\n","    # save current model state\n","    #if i % save_step == 0 and i > 0:\n","    #    model.save_pretrained(MODEL_SAVE_PATH)\n","    #    tokenizer.save_pretrained(MODEL_SAVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b4PYsIfga98"},"outputs":[],"source":["if api_key is not None:\n","  # this is needed in a notebook to finish the logs\n","  wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP4cuphAz9kVEPBq4V4z3RS","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
