{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEXr9X4hAh/xcW5nk0Xn3x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Apply the models on the Test set"],"metadata":{"id":"T7qZb9EAfxtW"}},{"cell_type":"code","source":["!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu -q"],"metadata":{"id":"fP8GD9YxtPV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from tqdm.auto import tqdm, trange\n","from transformers import AutoModelForSeq2SeqLM, NllbTokenizer"],"metadata":{"id":"KR8PMM9nhbeG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The project folder can be saved in google drive and accessed through Google colab\n","from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RudvEi7KhdK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####################\n","### Change here ####\n","####################\n","# path to the project folder that contains a folder data\n","path_project = \"/content/drive/MyDrive/German_to_Swiss_Translation_ANLP_2023\"\n","\n","# Path to where the models are stored\n","PATH_MODELS = \"/content/drive/MyDrive/anlp_project/NLLB-200/ANLP_SUBMISSION/models\"\n","\n","import os\n","os.chdir(path_project)"],"metadata":{"id":"-DmSpUVdhxgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translate(text, src_lang, tgt_lang, a=16, b=1.5, max_input_length=1024, **kwargs):\n","    tokenizer.src_lang = src_lang\n","    tokenizer.tgt_lang = tgt_lang\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n","    result = model.generate(\n","        **inputs.to(model.device),\n","        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n","        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n","        **kwargs\n","    )\n","    return tokenizer.batch_decode(result, skip_special_tokens=True)\n","\n","def batched_translate(texts, batch_size=16, **kwargs):\n","    \"\"\"Translate texts in batches of similar length\"\"\"\n","    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n","    results = []\n","    for i in trange(0, len(texts2), batch_size):\n","        results.extend(translate(texts2[i: i+batch_size], **kwargs))\n","    return [p for i, p in sorted(zip(idxs, results))]"],"metadata":{"id":"lpMk_rPRgpcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = pd.read_csv(f\"Data/df_test.csv\")"],"metadata":{"id":"rBFNbVJ8j4Zr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fix_tokenizer2n(tokenizer, new_lang_tokens):\n","    \"\"\"\n","    Add a new language token to the tokenizer vocabulary,\n","    this should be done each time after its initialization.\n","    This function is used when the model already contains another newly added token\n","    e.g. \"ch_vs\" or \"ch_gr\"\n","    \"\"\"\n","\n","    print(f\"BEFORE IDs: {tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>'])}\")\n","    print(f\"BEFORE Tokens: {tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>']))}\")\n","\n","    # how many of the tokens are actually in tokenizer.added_tokens_encoder\n","    n_added_tokens = len([token for token in new_lang_tokens if token in tokenizer.added_tokens_encoder])\n","\n","    # get the old/ original length of the tokenizer\n","    old_len = len(tokenizer) - n_added_tokens\n","\n","    # move the new tokens in the previous position\n","    for i, token in enumerate(new_lang_tokens):\n","      tokenizer.lang_code_to_id[token] = old_len-i\n","      tokenizer.id_to_lang_code[old_len-old_len-n_added_tokens-i] = token\n","\n","    # always move \"mask\" to the last position\n","    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n","\n","    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n","    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n","\n","    # if the token is not yet trained in the model, add it to the special tokens\n","    for token in new_lang_tokens:\n","      if token not in tokenizer._additional_special_tokens:\n","          tokenizer._additional_special_tokens.append(token)\n","\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    tokenizer.added_tokens_encoder = {}\n","    tokenizer.added_tokens_decoder = {}\n","\n","    print(f\"AFTER IDs: {tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>'])}\")\n","    print(f\"AFTER Tokens: {tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(new_lang_tokens[::-1] + ['<mask>']))}\")\n","\n","def fix_tokenizer1n(tokenizer, new_lang_token):\n","    \"\"\"\n","    Add a new language token to the tokenizer vocabulary,\n","    this should be done each time after its initialization.\n","    This function is used when the model does not contain another newly added token.\n","    \"\"\"\n","    old_len = len(tokenizer) - int(new_lang_token in tokenizer.added_tokens_encoder)\n","    tokenizer.lang_code_to_id[new_lang_token] = old_len-1\n","    tokenizer.id_to_lang_code[old_len-1] = new_lang_token\n","    # always move \"mask\" to the last position\n","    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n","\n","    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n","    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n","    if new_lang_token not in tokenizer._additional_special_tokens:\n","        tokenizer._additional_special_tokens.append(new_lang_token)\n","    # clear the added token encoder; otherwise a new token may end up there by mistake\n","    tokenizer.added_tokens_encoder = {}\n","    tokenizer.added_tokens_decoder = {}\n","\n","    print(f\"Length of the tokenizer: {len(tokenizer)}\")\n","    print(f\"IDs: {tokenizer.convert_tokens_to_ids([new_lang_token, '<mask>'])}\")\n","    print(f\"IDs: {tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids([new_lang_token, '<mask>']))}\")"],"metadata":{"id":"UIq_deZNszpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {\n","    \"init_none\" : [f\"{PATH_MODELS}/nllb-de-be_initNONE_best_checkpoint\", [\"ch_be\"]],\n","    \"init_vs\" : [f\"{PATH_MODELS}/nllb-de-be_initvs_best_checkpoint\", [\"ch_be\", \"ch_vs\"]],\n","    \"init_gr_large\" : [f\"{PATH_MODELS}/nllb-de-be_initgrLarge_best_checkpoint\", [\"ch_be\", \"ch_gr\"]],\n","    \"init_de\" :[f\"{PATH_MODELS}/nllb-de-be_initde_best_checkpoint\", [\"ch_be\"]],\n","    \"init_average\": [f\"{PATH_MODELS}/nllb-de-be_init_average_v1_best_checkpoint\", [\"ch_be\"]],\n","    \"init_gr_small\": [f\"{PATH_MODELS}/nllb-de-be_initgrSmall_best_checkpoint\", [\"ch_be\", \"ch_gr\"]]\n","}\n","\n","for key, value in models.items():\n","  model = AutoModelForSeq2SeqLM.from_pretrained(value[0])\n","  tokenizer = NllbTokenizer.from_pretrained(value[0])\n","\n","  if len(value[1]) == 2:\n","    fix_tokenizer2n(tokenizer, new_lang_tokens=value[1])\n","  elif len(value[1]) == 1:\n","    fix_tokenizer1n(tokenizer, new_lang_token=value[1][0])\n","\n","  df_test[key] = [translate(t, \"ch_be\", 'deu_Latn')[0] for t in tqdm(df_test[\"ch_be\"])]"],"metadata":{"id":"PVxyriRZf0ya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filename\n","#df_test.to_excel(\"df_test_output.xlsx\")"],"metadata":{"id":"pq4eq6jef_pj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Apply the model on the Synthetic Test Set"],"metadata":{"id":"e4NnoN7of4RJ"}},{"cell_type":"code","source":["# Path to the file that contains the synthetic source and target sentences\n","df_syn = pd.read_excel(\"Results/Manual_Syntax_Evaluation_Synt_TestSet.xlsx\")\n","df_syn = df_syn[[\"de\", \"be\"]]"],"metadata":{"id":"9ECkkceqf53I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key, value in models.items():\n","  model = AutoModelForSeq2SeqLM.from_pretrained(value[0])\n","  tokenizer = NllbTokenizer.from_pretrained(value[0])\n","\n","  if len(value[1]) == 2:\n","    fix_tokenizer2n(tokenizer, new_lang_tokens=value[1])\n","  elif len(value[1]) == 1:\n","    fix_tokenizer1n(tokenizer, new_lang_token=value[1][0])\n","\n","  df_test[key] = [translate(t, \"ch_be\", 'deu_Latn')[0] for t in tqdm(df_syn[\"be\"])]"],"metadata":{"id":"W5l4816e0JTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filename\n","#df_test.to_excel(\"df_syn_test_output.xlsx\")"],"metadata":{"id":"KiS3NNMg4apz"},"execution_count":null,"outputs":[]}]}