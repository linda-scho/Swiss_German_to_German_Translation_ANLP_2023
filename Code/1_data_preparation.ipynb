{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHUjg9KMuyGE0/mi4xGUHn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"z_ET_uPlkeoF"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# The project folder can be saved in google drive and accessed through Google colab\n","from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["####################\n","### Change here ####\n","####################\n","# path to the project folder that contains a folder data\n","path_project = \"/content/drive/MyDrive/German_to_Swiss_Translation_ANLP_2023\"\n","\n","import os\n","os.chdir(path_project)"],"metadata":{"id":"2zRLlDUzh-Kd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_data(path, save_path):\n","    \"\"\"\n","    Takes as input a path to a json file and saves train and test dataframes.\n","    \"\"\"\n","    test_frac=0.05\n","\n","    with open(path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","\n","    df = pd.DataFrame(data)\n","\n","    # print number of not null rows for each dialect and the total lenth of the df\n","    print(f\"Null values: {len(df) - df.isna().sum()}\")\n","    print(f\"Total lenth of the dataframe: {len(df)}\")\n","\n","    # get the sentences that have data for all 4 languages/ dialects we consider: 'de', 'ch_be', 'ch_gr', 'ch_vs'\n","    df_filtered = df[df[['de', 'ch_be', 'ch_gr', 'ch_vs']].notna().all(axis=1)].reset_index(drop=True)\n","\n","    # extraxt a fraction of these sentences as a testset so it won't be part of the embeddings in any dialect\n","    num_rows_to_sample = int(len(df_filtered) * test_frac)\n","\n","    # Randomly sample 0.05% of the rows\n","    df_test = df_filtered.sample(frac=0.05, random_state=42)\n","\n","    # The sampled rows are now in df_test, and you can remove them from the original DataFrame if needed\n","    df_train = df.drop(df_test.id)\n","\n","    # Reset the index of df_test if needed\n","    df_test.reset_index(drop=True, inplace=True)\n","\n","    # save the train and test data in the specified folder\n","    df_test.to_csv(f\"{save_path}/df_test.csv\")\n","    df_train.to_csv(f\"{save_path}/df_train.csv\")\n","    print(\"train and test data saved.\")\n","\n","    return df_train, df_test"],"metadata":{"id":"kOMUSvKydCzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_data_path = f\"Data/raw_data/sentences_ch_de_transcribed.json\"\n","save_path = f\"Data/\"\n","\n","df_train, df_test = read_data(raw_data_path, save_path)"],"metadata":{"id":"hL2u0JeLknGA"},"execution_count":null,"outputs":[]}]}