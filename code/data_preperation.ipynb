{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYIYKsr+FloqGKny278po5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"z_ET_uPlkeoF","executionInfo":{"status":"ok","timestamp":1703098355041,"user_tz":-60,"elapsed":796,"user":{"displayName":"Mira Metzger","userId":"01779271164224279332"}}},"outputs":[],"source":["import argparse\n","import pandas as pd\n","import json\n","\n","def read_data(path, save_path):\n","    \"\"\"\n","    Takes as input a path to a json file and saves train and test dataframes.\n","    \"\"\"\n","    test_frac=0.05\n","\n","    with open(path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","\n","    df = pd.DataFrame(data)\n","\n","    # print number of not null rows for each dialect and the total lenth of the df\n","    print(f\"Null values: {len(df) - df.isna().sum()}\")\n","    print(f\"Total lenth of the dataframe: {len(df)}\")\n","\n","    # get the sentences that have data for all 4 languages/ dialects we consider: 'de', 'ch_be', 'ch_gr', 'ch_vs'\n","    df_filtered = df[df[['de', 'ch_be', 'ch_gr', 'ch_vs']].notna().all(axis=1)].reset_index(drop=True)\n","\n","    # extraxt a fraction of these sentences as a testset so it won't be part of the embeddings in any dialect\n","    num_rows_to_sample = int(len(df_filtered) * test_frac)\n","\n","    # Randomly sample 0.05% of the rows\n","    df_test = df_filtered.sample(frac=0.05, random_state=42)\n","\n","    # The sampled rows are now in df_test, and you can remove them from the original DataFrame if needed\n","    df_train = df.drop(df_test.id)\n","\n","    # Reset the index of df_test if needed\n","    df_test.reset_index(drop=True, inplace=True)\n","\n","    # save the train and test data in the specified folder\n","    df_test.to_csv(f\"{save_path}/df_test.csv\")\n","    df_train.to_csv(f\"{save_path}/df_train.csv\")\n","    print(\"train and test data saved.\")\n","\n","    return df_train, df_test"]},{"cell_type":"code","source":["# The project folder can be saved in google drive and accessed through Google colab\n","from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMeHavSnk2vQ","executionInfo":{"status":"ok","timestamp":1703098441071,"user_tz":-60,"elapsed":24970,"user":{"displayName":"Mira Metzger","userId":"01779271164224279332"}},"outputId":"609fc6fc-0fea-4937-9014-813705db85cf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# path to the project folder that contains a folder code, data and models\n","path_project = \"/content/drive/MyDrive/anlp_project/NLLB-200/ANLP_SUBMISSION\"\n","\n","raw_data_path = f\"{path_project}/data/raw_data/sentences_ch_de_transcribed.json\"\n","save_path = f\"{path_project}/data/\"\n","\n","df_train, df_test = read_data(raw_data_path, save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hL2u0JeLknGA","executionInfo":{"status":"ok","timestamp":1703098696964,"user_tz":-60,"elapsed":325,"user":{"displayName":"Mira Metzger","userId":"01779271164224279332"}},"outputId":"5191aa6d-f2d8-4565-fd01-656829a2bdcb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Null values: id                11213\n","de                11213\n","ch_sg              2752\n","ch_be              2700\n","ch_gr             10475\n","ch_zh              4065\n","ch_vs              2753\n","ch_bs              2713\n","ch_ag              2748\n","ch_lu              2715\n","thema             11213\n","code_switching      351\n","dtype: int64\n","Total lenth of the dataframe: 11213\n","train and test data saved.\n"]}]}]}